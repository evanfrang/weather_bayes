{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14eac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.optim import Adam\n",
    "from pyro.util import ignore_jit_warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedb2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(relativeCreated) 9d %(message)s\", level=logging.DEBUG)\n",
    "\n",
    "# Add another handler for logging debugging events (e.g. for profiling)\n",
    "# in a separate stream that can be captured.\n",
    "log = logging.getLogger()\n",
    "debug_handler = logging.StreamHandler(sys.stdout)\n",
    "debug_handler.setLevel(logging.DEBUG)\n",
    "debug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\n",
    "log.addHandler(debug_handler)\n",
    "\n",
    "\n",
    "# Let's start with a simple Hidden Markov Model.\n",
    "#\n",
    "#     x[t-1] --> x[t] --> x[t+1]\n",
    "#        |        |         |\n",
    "#        V        V         V\n",
    "#     y[t-1]     y[t]     y[t+1]\n",
    "#\n",
    "# This model includes a plate for the data_dim = 88 keys on the piano. This\n",
    "# model has two \"style\" parameters probs_x and probs_y that we'll draw from a\n",
    "# prior. The latent state is x, and the observed state is y. We'll drive\n",
    "# probs_* with the guide, enumerate over x, and condition on y.\n",
    "#\n",
    "# Importantly, the dependency structure of the enumerated variables has\n",
    "# narrow treewidth, therefore admitting efficient inference by message passing.\n",
    "# Pyro's TraceEnum_ELBO will find an efficient message passing scheme if one\n",
    "# exists.\n",
    "def model_0(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    assert not torch._C._get_tracing_state()\n",
    "    num_sequences, max_length, data_dim = sequences.shape\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        # Our prior on transition probabilities will be:\n",
    "        # stay in the same state with 90% probability; uniformly jump to another\n",
    "        # state with 10% probability.\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\",\n",
    "            dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1).to_event(1),\n",
    "        )\n",
    "        # We put a weak prior on the conditional probability of a tone sounding.\n",
    "        # We know that on average about 4 of 88 tones are active, so we'll set a\n",
    "        # rough weak prior of 10% of the notes being active at any one time.\n",
    "        probs_y = pyro.sample(\n",
    "            \"probs_y\",\n",
    "            dist.Beta(0.1, 0.9).expand([args.hidden_dim, data_dim]).to_event(2),\n",
    "        )\n",
    "    # In this first model we'll sequentially iterate over sequences in a\n",
    "    # minibatch; this will make it easy to reason about tensor shapes.\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    for i in pyro.plate(\"sequences\", len(sequences), batch_size):\n",
    "        length = lengths[i]\n",
    "        sequence = sequences[i, :length]\n",
    "        x = 0\n",
    "        for t in pyro.markov(range(length)):\n",
    "            # On the next line, we'll overwrite the value of x with an updated\n",
    "            # value. If we wanted to record all x values, we could instead\n",
    "            # write x[t] = pyro.sample(...x[t-1]...).\n",
    "            x = pyro.sample(\n",
    "                \"x_{}_{}\".format(i, t),\n",
    "                dist.Categorical(probs_x[x]),\n",
    "                infer={\"enumerate\": \"parallel\"},\n",
    "            )\n",
    "            with tones_plate:\n",
    "                pyro.sample(\n",
    "                    \"y_{}_{}\".format(i, t),\n",
    "                    dist.Bernoulli(probs_y[x.squeeze(-1)]),\n",
    "                    obs=sequence[t],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e37e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    # Sometimes it is safe to ignore jit warnings. Here we use the\n",
    "    # pyro.util.ignore_jit_warnings context manager to silence warnings about\n",
    "    # conversion to integer, since we know all three numbers will be the same\n",
    "    # across all invocations to the model.\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\",\n",
    "            dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1).to_event(1),\n",
    "        )\n",
    "        probs_y = pyro.sample(\n",
    "            \"probs_y\",\n",
    "            dist.Beta(0.1, 0.9).expand([args.hidden_dim, data_dim]).to_event(2),\n",
    "        )\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    # We subsample batch_size items out of num_sequences items. Note that since\n",
    "    # we're using dim=-1 for the notes plate, we need to batch over a different\n",
    "    # dimension, here dim=-2.\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x = 0\n",
    "        # If we are not using the jit, then we can vary the program structure\n",
    "        # each call by running for a dynamically determined number of time\n",
    "        # steps, lengths.max(). However if we are using the jit, then we try to\n",
    "        # keep a single program structure for all minibatches; the fixed\n",
    "        # structure ends up being faster since each program structure would\n",
    "        # need to trigger a new jit compile stage.\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                x = pyro.sample(\n",
    "                    \"x_{}\".format(t),\n",
    "                    dist.Categorical(probs_x[x]),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                with tones_plate:\n",
    "                    pyro.sample(\n",
    "                        \"y_{}\".format(t),\n",
    "                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\n",
    "                        obs=sequences[batch, t],\n",
    "                    )\n",
    "\n",
    "\n",
    "# Let's see how batching changes the shapes of sample sites:\n",
    "# $ python examples/hmm.py -m 1 -n 1 -t 5 --batch-size=10 --print-shapes\n",
    "# ...\n",
    "#  Sample Sites:\n",
    "#   probs_x dist             | 16 16\n",
    "#          value             | 16 16\n",
    "#   probs_y dist             | 16 88\n",
    "#          value             | 16 88\n",
    "#     tones dist             |\n",
    "#          value          88 |\n",
    "# sequences dist             |\n",
    "#          value          10 |\n",
    "#       x_0 dist       10  1 |\n",
    "#          value    16  1  1 |\n",
    "#       y_0 dist    16 10 88 |\n",
    "#          value       10 88 |\n",
    "#       x_1 dist    16 10  1 |\n",
    "#          value 16  1  1  1 |\n",
    "#       y_1 dist 16  1 10 88 |\n",
    "#          value       10 88 |\n",
    "#       x_2 dist 16  1 10  1 |\n",
    "#          value    16  1  1 |\n",
    "#       y_2 dist    16 10 88 |\n",
    "#          value       10 88 |\n",
    "#       x_3 dist    16 10  1 |\n",
    "#          value 16  1  1  1 |\n",
    "#       y_3 dist 16  1 10 88 |\n",
    "#          value       10 88 |\n",
    "#       x_4 dist 16  1 10  1 |\n",
    "#          value    16  1  1 |\n",
    "#       y_4 dist    16 10 88 |\n",
    "#          value       10 88 |\n",
    "#\n",
    "# Notice that we're now using dim=-2 as a batch dimension (of size 10),\n",
    "# and that the enumeration dimensions are now dims -3 and -4.\n",
    "\n",
    "\n",
    "# Next let's add a dependency of y[t] on y[t-1].\n",
    "#\n",
    "#     x[t-1] --> x[t] --> x[t+1]\n",
    "#        |        |         |\n",
    "#        V        V         V\n",
    "#     y[t-1] --> y[t] --> y[t+1]\n",
    "#\n",
    "# Note that this is the \"arHMM\" model in reference [1].\n",
    "def model_2(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\",\n",
    "            dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1).to_event(1),\n",
    "        )\n",
    "        probs_y = pyro.sample(\n",
    "            \"probs_y\",\n",
    "            dist.Beta(0.1, 0.9).expand([args.hidden_dim, 2, data_dim]).to_event(3),\n",
    "        )\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x, y = 0, 0\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                x = pyro.sample(\n",
    "                    \"x_{}\".format(t),\n",
    "                    dist.Categorical(probs_x[x]),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                # Note the broadcasting tricks here: to index probs_y on tensors x and y,\n",
    "                # we also need a final tensor for the tones dimension. This is conveniently\n",
    "                # provided by the plate associated with that dimension.\n",
    "                with tones_plate as tones:\n",
    "                    y = pyro.sample(\n",
    "                        \"y_{}\".format(t),\n",
    "                        dist.Bernoulli(probs_y[x, y, tones]),\n",
    "                        obs=sequences[batch, t],\n",
    "                    ).long()\n",
    "\n",
    "\n",
    "# Next consider a Factorial HMM with two hidden states.\n",
    "#\n",
    "#    w[t-1] ----> w[t] ---> w[t+1]\n",
    "#        \\ x[t-1] --\\-> x[t] --\\-> x[t+1]\n",
    "#         \\  /       \\  /       \\  /\n",
    "#          \\/         \\/         \\/\n",
    "#        y[t-1]      y[t]      y[t+1]\n",
    "#\n",
    "# Note that since the joint distribution of each y[t] depends on two variables,\n",
    "# those two variables become dependent. Therefore during enumeration, the\n",
    "# entire joint space of these variables w[t],x[t] needs to be enumerated.\n",
    "# For that reason, we set the dimension of each to the square root of the\n",
    "# target hidden dimension.\n",
    "#\n",
    "# Note that this is the \"FHMM\" model in reference [1].\n",
    "def model_3(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    hidden_dim = int(args.hidden_dim**0.5)  # split between w and x\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_w = pyro.sample(\n",
    "            \"probs_w\", dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1).to_event(1)\n",
    "        )\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\", dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1).to_event(1)\n",
    "        )\n",
    "        probs_y = pyro.sample(\n",
    "            \"probs_y\",\n",
    "            dist.Beta(0.1, 0.9).expand([hidden_dim, hidden_dim, data_dim]).to_event(3),\n",
    "        )\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        w, x = 0, 0\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                w = pyro.sample(\n",
    "                    \"w_{}\".format(t),\n",
    "                    dist.Categorical(probs_w[w]),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                x = pyro.sample(\n",
    "                    \"x_{}\".format(t),\n",
    "                    dist.Categorical(probs_x[x]),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                with tones_plate as tones:\n",
    "                    pyro.sample(\n",
    "                        \"y_{}\".format(t),\n",
    "                        dist.Bernoulli(probs_y[w, x, tones]),\n",
    "                        obs=sequences[batch, t],\n",
    "                    )\n",
    "\n",
    "\n",
    "# By adding a dependency of x on w, we generalize to a\n",
    "# Dynamic Bayesian Network.\n",
    "#\n",
    "#     w[t-1] ----> w[t] ---> w[t+1]\n",
    "#        |  \\       |  \\       |   \\\n",
    "#        | x[t-1] ----> x[t] ----> x[t+1]\n",
    "#        |   /      |   /      |   /\n",
    "#        V  /       V  /       V  /\n",
    "#     y[t-1]       y[t]      y[t+1]\n",
    "#\n",
    "# Note that message passing here has roughly the same cost as with the\n",
    "# Factorial HMM, but this model has more parameters.\n",
    "#\n",
    "# Note that this is the \"PFHMM\" model in reference [1].\n",
    "def model_4(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    hidden_dim = int(args.hidden_dim**0.5)  # split between w and x\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_w = pyro.sample(\n",
    "            \"probs_w\", dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1).to_event(1)\n",
    "        )\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\",\n",
    "            dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n",
    "            .expand_by([hidden_dim])\n",
    "            .to_event(2),\n",
    "        )\n",
    "        probs_y = pyro.sample(\n",
    "            \"probs_y\",\n",
    "            dist.Beta(0.1, 0.9).expand([hidden_dim, hidden_dim, data_dim]).to_event(3),\n",
    "        )\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        # Note the broadcasting tricks here: we declare a hidden torch.arange and\n",
    "        # ensure that w and x are always tensors so we can unsqueeze them below,\n",
    "        # thus ensuring that the x sample sites have correct distribution shape.\n",
    "        w = x = torch.tensor(0, dtype=torch.long)\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                w = pyro.sample(\n",
    "                    \"w_{}\".format(t),\n",
    "                    dist.Categorical(probs_w[w]),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                x = pyro.sample(\n",
    "                    \"x_{}\".format(t),\n",
    "                    dist.Categorical(Vindex(probs_x)[w, x]),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                with tones_plate as tones:\n",
    "                    pyro.sample(\n",
    "                        \"y_{}\".format(t),\n",
    "                        dist.Bernoulli(probs_y[w, x, tones]),\n",
    "                        obs=sequences[batch, t],\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TonesGenerator(nn.Module):\n",
    "    def __init__(self, args, data_dim):\n",
    "        self.args = args\n",
    "        self.data_dim = data_dim\n",
    "        super().__init__()\n",
    "        self.x_to_hidden = nn.Linear(args.hidden_dim, args.nn_dim)\n",
    "        self.y_to_hidden = nn.Linear(args.nn_channels * data_dim, args.nn_dim)\n",
    "        self.conv = nn.Conv1d(1, args.nn_channels, 3, padding=1)\n",
    "        self.hidden_to_logits = nn.Linear(args.nn_dim, data_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Hidden units depend on two inputs: a one-hot encoded categorical variable x, and\n",
    "        # a bernoulli variable y. Whereas x will typically be enumerated, y will be observed.\n",
    "        # We apply x_to_hidden independently from y_to_hidden, then broadcast the non-enumerated\n",
    "        # y part up to the enumerated x part in the + operation.\n",
    "        x_onehot = y.new_zeros(x.shape[:-1] + (self.args.hidden_dim,)).scatter_(\n",
    "            -1, x, 1\n",
    "        )\n",
    "        y_conv = self.relu(self.conv(y.reshape(-1, 1, self.data_dim))).reshape(\n",
    "            y.shape[:-1] + (-1,)\n",
    "        )\n",
    "        h = self.relu(self.x_to_hidden(x_onehot) + self.y_to_hidden(y_conv))\n",
    "        return self.hidden_to_logits(h)\n",
    "\n",
    "\n",
    "# We will create a single global instance later.\n",
    "tones_generator = None\n",
    "\n",
    "\n",
    "# The neural HMM model now uses tones_generator at each time step.\n",
    "#\n",
    "# Note that this is the \"nnHMM\" model in reference [1].\n",
    "def model_5(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "\n",
    "    # Initialize a global module instance if needed.\n",
    "    global tones_generator\n",
    "    if tones_generator is None:\n",
    "        tones_generator = TonesGenerator(args, data_dim)\n",
    "    pyro.module(\"tones_generator\", tones_generator)\n",
    "\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\",\n",
    "            dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1).to_event(1),\n",
    "        )\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x = 0\n",
    "        y = torch.zeros(data_dim)\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                x = pyro.sample(\n",
    "                    \"x_{}\".format(t),\n",
    "                    dist.Categorical(probs_x[x]),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                # Note that since each tone depends on all tones at a previous time step\n",
    "                # the tones at different time steps now need to live in separate plates.\n",
    "                with pyro.plate(\"tones_{}\".format(t), data_dim, dim=-1):\n",
    "                    y = pyro.sample(\n",
    "                        \"y_{}\".format(t),\n",
    "                        dist.Bernoulli(logits=tones_generator(x, y)),\n",
    "                        obs=sequences[batch, t],\n",
    "                    )\n",
    "\n",
    "\n",
    "# Next let's consider a second-order HMM model\n",
    "# in which x[t+1] depends on both x[t] and x[t-1].\n",
    "#\n",
    "#                     _______>______\n",
    "#         _____>_____/______        \\\n",
    "#        /          /       \\        \\\n",
    "#     x[t-1] --> x[t] --> x[t+1] --> x[t+2]\n",
    "#        |        |          |          |\n",
    "#        V        V          V          V\n",
    "#     y[t-1]     y[t]     y[t+1]     y[t+2]\n",
    "#\n",
    "#  Note that in this model (in contrast to the previous model) we treat\n",
    "#  the transition and emission probabilities as parameters (so they have no prior).\n",
    "#\n",
    "# Note that this is the \"2HMM\" model in reference [1].\n",
    "def model_6(sequences, lengths, args, batch_size=None, include_prior=False):\n",
    "    num_sequences, max_length, data_dim = sequences.shape\n",
    "    assert lengths.shape == (num_sequences,)\n",
    "    assert lengths.max() <= max_length\n",
    "    hidden_dim = args.hidden_dim\n",
    "\n",
    "    if not args.raftery_parameterization:\n",
    "        # Explicitly parameterize the full tensor of transition probabilities, which\n",
    "        # has hidden_dim cubed entries.\n",
    "        probs_x = pyro.param(\n",
    "            \"probs_x\",\n",
    "            torch.rand(hidden_dim, hidden_dim, hidden_dim),\n",
    "            constraint=constraints.simplex,\n",
    "        )\n",
    "    else:\n",
    "        # Use the more parsimonious \"Raftery\" parameterization of\n",
    "        # the tensor of transition probabilities. See reference:\n",
    "        # Raftery, A. E. A model for high-order markov chains.\n",
    "        # Journal of the Royal Statistical Society. 1985.\n",
    "        probs_x1 = pyro.param(\n",
    "            \"probs_x1\",\n",
    "            torch.rand(hidden_dim, hidden_dim),\n",
    "            constraint=constraints.simplex,\n",
    "        )\n",
    "        probs_x2 = pyro.param(\n",
    "            \"probs_x2\",\n",
    "            torch.rand(hidden_dim, hidden_dim),\n",
    "            constraint=constraints.simplex,\n",
    "        )\n",
    "        mix_lambda = pyro.param(\n",
    "            \"mix_lambda\", torch.tensor(0.5), constraint=constraints.unit_interval\n",
    "        )\n",
    "        # we use broadcasting to combine two tensors of shape (hidden_dim, hidden_dim) and\n",
    "        # (hidden_dim, 1, hidden_dim) to obtain a tensor of shape (hidden_dim, hidden_dim, hidden_dim)\n",
    "        probs_x = mix_lambda * probs_x1 + (1.0 - mix_lambda) * probs_x2.unsqueeze(-2)\n",
    "\n",
    "    probs_y = pyro.param(\n",
    "        \"probs_y\",\n",
    "        torch.rand(hidden_dim, data_dim),\n",
    "        constraint=constraints.unit_interval,\n",
    "    )\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x_curr, x_prev = torch.tensor(0), torch.tensor(0)\n",
    "        # we need to pass the argument `history=2' to `pyro.markov()`\n",
    "        # since our model is now 2-markov\n",
    "        for t in pyro.markov(range(lengths.max()), history=2):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                probs_x_t = Vindex(probs_x)[x_prev, x_curr]\n",
    "                x_prev, x_curr = x_curr, pyro.sample(\n",
    "                    \"x_{}\".format(t),\n",
    "                    dist.Categorical(probs_x_t),\n",
    "                    infer={\"enumerate\": \"parallel\"},\n",
    "                )\n",
    "                with tones_plate:\n",
    "                    probs_y_t = probs_y[x_curr.squeeze(-1)]\n",
    "                    pyro.sample(\n",
    "                        \"y_{}\".format(t),\n",
    "                        dist.Bernoulli(probs_y_t),\n",
    "                        obs=sequences[batch, t],\n",
    "                    )\n",
    "\n",
    "\n",
    "# Next we demonstrate how to parallelize the neural HMM above using Pyro's\n",
    "# DiscreteHMM distribution. This model is equivalent to model_5 above, but we\n",
    "# manually unroll loops and fuse ops, leading to a single sample statement.\n",
    "# DiscreteHMM can lead to over 10x speedup in models where it is applicable.\n",
    "def model_7(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "\n",
    "    # Initialize a global module instance if needed.\n",
    "    global tones_generator\n",
    "    if tones_generator is None:\n",
    "        tones_generator = TonesGenerator(args, data_dim)\n",
    "    pyro.module(\"tones_generator\", tones_generator)\n",
    "\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\",\n",
    "            dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1).to_event(1),\n",
    "        )\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-1) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        y = sequences[batch] if args.jit else sequences[batch, : lengths.max()]\n",
    "        x = torch.arange(args.hidden_dim)\n",
    "        t = torch.arange(y.size(1))\n",
    "        init_logits = torch.full((args.hidden_dim,), -float(\"inf\"))\n",
    "        init_logits[0] = 0\n",
    "        trans_logits = probs_x.log()\n",
    "        with ignore_jit_warnings():\n",
    "            obs_dist = dist.Bernoulli(\n",
    "                logits=tones_generator(x, y.unsqueeze(-2))\n",
    "            ).to_event(1)\n",
    "            obs_dist = obs_dist.mask((t < lengths.unsqueeze(-1)).unsqueeze(-1))\n",
    "            hmm_dist = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n",
    "        pyro.sample(\"y\", hmm_dist, obs=y)\n",
    "\n",
    "\n",
    "models = {\n",
    "    name[len(\"model_\") :]: model\n",
    "    for name, model in globals().items()\n",
    "    if name.startswith(\"model_\")\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.cuda:\n",
    "        torch.set_default_device(\"cuda\")\n",
    "\n",
    "    logging.info(\"Loading data\")\n",
    "    #data = poly.load_data(poly.JSB_CHORALES)\n",
    "    with open('jsbchorales.pkl', 'rb') as p:\n",
    "        data = pickle.load(p, encoding=\"latin1\")\n",
    "\n",
    "    logging.info(\"-\" * 40)\n",
    "    model = models[args.model]\n",
    "    logging.info(\n",
    "        \"Training {} on {} sequences\".format(\n",
    "            model.__name__, len(data['train'])\n",
    "        )\n",
    "    )\n",
    "    sequences = data['train']\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # find all the notes that are present at least once in the training set\n",
    "    present_notes = (sequences == 1).sum(0).sum(0) > 0\n",
    "    # remove notes that are never played (we remove 37/88 notes)\n",
    "    sequences = sequences[..., present_notes]\n",
    "\n",
    "    if args.truncate:\n",
    "        lengths = lengths.clamp(max=args.truncate)\n",
    "        sequences = sequences[:, : args.truncate]\n",
    "    num_observations = float(lengths.sum())\n",
    "    pyro.set_rng_seed(args.seed)\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # We'll train using MAP Baum-Welch, i.e. MAP estimation while marginalizing\n",
    "    # out the hidden state x. This is accomplished via an automatic guide that\n",
    "    # learns point estimates of all of our conditional probability tables,\n",
    "    # named probs_*.\n",
    "    guide = AutoDelta(\n",
    "        poutine.block(model, expose_fn=lambda msg: msg[\"name\"].startswith(\"probs_\"))\n",
    "    )\n",
    "\n",
    "    # To help debug our tensor shapes, let's print the shape of each site's\n",
    "    # distribution, value, and log_prob tensor. Note this information is\n",
    "    # automatically printed on most errors inside SVI.\n",
    "    if args.print_shapes:\n",
    "        first_available_dim = -2 if model is model_0 else -3\n",
    "        guide_trace = poutine.trace(guide).get_trace(\n",
    "            sequences, lengths, args=args, batch_size=args.batch_size\n",
    "        )\n",
    "        model_trace = poutine.trace(\n",
    "            poutine.replay(poutine.enum(model, first_available_dim), guide_trace)\n",
    "        ).get_trace(sequences, lengths, args=args, batch_size=args.batch_size)\n",
    "        logging.info(model_trace.format_shapes())\n",
    "\n",
    "    # Enumeration requires a TraceEnum elbo and declaring the max_plate_nesting.\n",
    "    # All of our models have two plates: \"data\" and \"tones\".\n",
    "    optim = Adam({\"lr\": args.learning_rate})\n",
    "    if args.tmc:\n",
    "        if args.jit:\n",
    "            raise NotImplementedError(\"jit support not yet added for TraceTMC_ELBO\")\n",
    "        elbo = TraceTMC_ELBO(max_plate_nesting=1 if model is model_0 else 2)\n",
    "        tmc_model = poutine.infer_config(\n",
    "            model,\n",
    "            lambda msg: (\n",
    "                {\"num_samples\": args.tmc_num_samples, \"expand\": False}\n",
    "                if msg[\"infer\"].get(\"enumerate\", None) == \"parallel\"\n",
    "                else {}\n",
    "            ),\n",
    "        )  # noqa: E501\n",
    "        svi = SVI(tmc_model, guide, optim, elbo)\n",
    "    else:\n",
    "        Elbo = JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO\n",
    "        elbo = Elbo(\n",
    "            max_plate_nesting=1 if model is model_0 else 2,\n",
    "            strict_enumeration_warning=(model is not model_7),\n",
    "            jit_options={\"time_compilation\": args.time_compilation},\n",
    "        )\n",
    "        svi = SVI(model, guide, optim, elbo)\n",
    "\n",
    "    # We'll train on small minibatches.\n",
    "    logging.info(\"Step\\tLoss\")\n",
    "    for step in range(args.num_steps):\n",
    "        loss = svi.step(sequences, lengths, args=args, batch_size=args.batch_size)\n",
    "        logging.info(\"{: >5d}\\t{}\".format(step, loss / num_observations))\n",
    "\n",
    "    if args.jit and args.time_compilation:\n",
    "        logging.debug(\n",
    "            \"time to compile: {} s.\".format(elbo._differentiable_loss.compile_time)\n",
    "        )\n",
    "\n",
    "    # We evaluate on the entire training dataset,\n",
    "    # excluding the prior term so our results are comparable across models.\n",
    "    train_loss = elbo.loss(model, guide, sequences, lengths, args, include_prior=False)\n",
    "    logging.info(\"training loss = {}\".format(train_loss / num_observations))\n",
    "\n",
    "    # Finally we evaluate on the test dataset.\n",
    "    logging.info(\"-\" * 40)\n",
    "    logging.info(\n",
    "        \"Evaluating on {} test sequences\".format(len(data['test']))\n",
    "    )\n",
    "    #sequences = data[\"test\"][\"sequences\"][..., present_notes]\n",
    "    #lengths = data[\"test\"][\"sequence_lengths\"]\n",
    "    sequences = data['test']\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    if args.truncate:\n",
    "        lengths = lengths.clamp(max=args.truncate)\n",
    "    num_observations = float(lengths.sum())\n",
    "\n",
    "    # note that since we removed unseen notes above (to make the problem a bit easier and for\n",
    "    # numerical stability) this test loss may not be directly comparable to numbers\n",
    "    # reported on this dataset elsewhere.\n",
    "    test_loss = elbo.loss(\n",
    "        model, guide, sequences, lengths, args=args, include_prior=False\n",
    "    )\n",
    "    logging.info(\"test loss = {}\".format(test_loss / num_observations))\n",
    "\n",
    "    # We expect models with higher capacity to perform better,\n",
    "    # but eventually overfit to the training set.\n",
    "    capacity = sum(\n",
    "        value.reshape(-1).size(0) for value in pyro.get_param_store().values()\n",
    "    )\n",
    "    logging.info(\"{} capacity = {} parameters\".format(model.__name__, capacity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "463aa35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   714129 Loading data\n",
      "   714225 ----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'train', 'valid'])\n",
      "229\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--tmc-num-samples\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     36\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(args\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m---> 37\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     13\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m models[args\u001b[38;5;241m.\u001b[39mmodel]\n\u001b[0;32m     15\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m---> 17\u001b[0m         model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m sequences \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m lengths \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    assert pyro.__version__.startswith(\"1.9.1\")\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"MAP Baum-Welch learning Bach Chorales\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"-m\",\n",
    "        \"--model\",\n",
    "        default=\"1\",\n",
    "        type=str,\n",
    "        help=\"one of: {}\".format(\", \".join(sorted(models.keys()))),\n",
    "    )\n",
    "    parser.add_argument(\"-n\", \"--num-steps\", default=50, type=int)\n",
    "    parser.add_argument(\"-b\", \"--batch-size\", default=8, type=int)\n",
    "    parser.add_argument(\"-d\", \"--hidden-dim\", default=16, type=int)\n",
    "    parser.add_argument(\"-nn\", \"--nn-dim\", default=48, type=int)\n",
    "    parser.add_argument(\"-nc\", \"--nn-channels\", default=2, type=int)\n",
    "    parser.add_argument(\"-lr\", \"--learning-rate\", default=0.05, type=float)\n",
    "    parser.add_argument(\"-t\", \"--truncate\", type=int)\n",
    "    parser.add_argument(\"-p\", \"--print-shapes\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--cuda\", action=\"store_true\")\n",
    "    parser.add_argument(\"--jit\", action=\"store_true\")\n",
    "    parser.add_argument(\"--time-compilation\", action=\"store_true\")\n",
    "    parser.add_argument(\"-rp\", \"--raftery-parameterization\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--tmc\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Use Tensor Monte Carlo instead of exact enumeration \"\n",
    "        \"to estimate the marginal likelihood. You probably don't want to do this, \"\n",
    "        \"except to see that TMC makes Monte Carlo gradient estimation feasible \"\n",
    "        \"even with very large numbers of non-reparametrized variables.\",\n",
    "    )\n",
    "    parser.add_argument(\"--tmc-num-samples\", default=10, type=int)\n",
    "    args = parser.parse_args(args=[])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef6963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
